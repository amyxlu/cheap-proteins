{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "543328ce-b136-4cae-aa85-78b42c9d6c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8be55e19-3879-4fea-a1c1-be28ab66f75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c716641-1dd3-49c7-a0f4-c25056ec866a",
   "metadata": {},
   "source": [
    "# Load HPCT Compression Model Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72bdcf06-769d-4383-ab68-a3fd945538c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tanh layer at bottleneck...\n",
      "Finished loading HPCT model with shorten factor 2 and 64 channel dimensions.\n"
     ]
    }
   ],
   "source": [
    "from cheap.pretrained import CHEAP_shorten_2_dim_64\n",
    "\n",
    "model = CHEAP_shorten_2_dim_64()\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d806b945-c1c4-4f7a-a478-5f813eb951dc",
   "metadata": {},
   "source": [
    "# Obtain Uncompressed $p(\\text{sequence}, \\text{structure})$ Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8735c72d-7f94-426c-b744-0dd42be4af61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ESMFold embedding only model...\n",
      "ESMFold embedding only model created in 31.72 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load ESMFold Embedding-Only Model\n",
    "from cheap.esmfold import esmfold_v1_embed_only\n",
    "\n",
    "esmfold_embedder = esmfold_v1_embed_only()\n",
    "_ = esmfold_embedder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191a309f-99ed-4d13-9438-61ff18d79982",
   "metadata": {},
   "source": [
    "Alternatively, if the embeddings were already precomputed, this datamodule loaders the pre-computed embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16202668-3b04-4bf6-90e3-c73306e78cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cheap.datasets import H5DataModule\n",
    "\n",
    "# dm = H5DataModule(\"/data/lux70/data/cath/shards\")\n",
    "# dm.setup()\n",
    "# val_dataloader = dm.val_dataloader()\n",
    "# batch = next(iter(val_dataloader))\n",
    "\n",
    "# x, sequences, headers = batch\n",
    "\n",
    "# from cheap.esmfold import batch_encode_sequences\n",
    "# aatype, mask, _, _, _ = batch_encode_sequences(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c96a81b0-6094-466c-8d9c-3a73e24cc4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\n",
    "    # >cath|current|12asA00/4-330\n",
    "    \"AYIAKQRQISFVKSHFSRQLEERLGLIEVQAPILSRVGDGTQDNLSGAEKAVQVKVKALPDAQFEVVHSLAKWKRQTLGQHDFSAGEGLYTHMKALRPDEDRLSPLHSVYVDQWDWERVMGDGERQFSTLKSTVEAIWAGIKATEAAVSEEFGLAPFLPDQIHFVHSQELLSRYPDLDAKGRERAIAKDLGAVFLVGIGGKLSDGHRHDVRAPDYDDWSTPSELGHAGLNGDILVWNPVLEDAFELSSMGIRVDADTLKHQLALTGDEDRLELEWHQALLRGEMPQTIGGGIGQSRLTMLLLQLPHIGQVQAGVWPAAVRESVPSLL\",\n",
    "    # >cath|current|132lA00/2-129\n",
    "    \"VFGRCELAAAMRHGLDNYRGYSLGNWVCAAFESNFNTQATNRNTDGSTDYGILQINSRWWCNDGRTPGSRNLCNIPCSALLSSDITASVNCAKIVSDGNGMNAWVAWRNRCGTDVQAWIRGCRL\",\n",
    "    # >cath|current|153lA00/1-185\n",
    "    \"RTDCYGNVNRIDTTGASCKTAKPEGLSYCGVSASKKIAERDLQAMDRYKTIIKKVGEKLCVEPAVIAGIISRESHAGKVLKNGWGDRGNGFGLMQVDKRSHKPQGTWNGEVHITQGTTILINFIKTIQKKFPSWTKDQQLKGGISAYNAGAGNVRSYARMDIGTTHDDYANDVVARAQYYKQHGY\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d916576d-6ac2-4e6e-8a76-195030a58552",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = esmfold_embedder.infer_embedding(sequences)\n",
    "emb, mask = res['s'], res['mask']\n",
    "emb, mask = emb.to(device), mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38e9c58d-385e-44e3-ae19-cb255aeb69eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 327, 1024])\n",
      "torch.Size([3, 327])\n"
     ]
    }
   ],
   "source": [
    "print(emb.shape)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b659582b-3e7e-4da5-8fee-728958dac24b",
   "metadata": {},
   "source": [
    "The sequences have been automatically padded to the longest sequence in the batch.\n",
    "If you are running out of memory, consider trimming the sequences prior to this embedding inference operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8860b67f-ee3d-410e-8add-d39a93ccd403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([327, 124, 185], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sequence_lengths = mask.sum(dim=1)\n",
    "print(sequence_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20014485-11f9-4b83-9cab-d5c7e646f694",
   "metadata": {},
   "source": [
    "## Sanity Check Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb72b086-cab0-4cde-850b-222596951bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating ESMFold...\n",
      "ESMFold model loaded in 37.51 seconds.\n"
     ]
    }
   ],
   "source": [
    "from cheap.proteins import LatentToStructure, LatentToSequence\n",
    "\n",
    "# !!!! important !!!!\n",
    "# this implicitly creates the ESMFold model again, which might take up a lot of memory.\n",
    "# Must explicitly call `.to(device)` on this wrapper API; models are by default loaded to the CPU. \n",
    "latent_to_structure = LatentToStructure()\n",
    "latent_to_structure.to(device)\n",
    "\n",
    "# Similarly, this creates the sequence decoder, but it is much smaller than the structure decoder\n",
    "latent_to_sequence = LatentToSequence()\n",
    "latent_to_sequence = latent_to_sequence.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3c00d3-50e7-4789-ae33-25ba113577b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = latent_to_sequence.to_sequence(emb); assert len(res) == 3\n",
    "\n",
    "sanity_check_sequences = res[-1]\n",
    "for i, s in enumerate(sanity_check_sequences):\n",
    "    sanity_check_sequences[i] = s[:sequence_lengths[i]]\n",
    "\n",
    "print(sanity_check_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69afe8fc-f5d9-4d6a-9d7c-e8b497e7bab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plaid.utils import calc_sequence_recovery\n",
    "\n",
    "# sequence decoder gets perfect decoding accuracy:\n",
    "for orig_s, sanity_s in zip(sequences, sanity_check_sequences):\n",
    "    print(calc_sequence_recovery(orig_s, sanity_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c123ae5-0639-42b9-896e-ebf0a4eb29d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also get the structures -- we will visualize this together with the reconstructions later.\n",
    "sanity_pdb_strs, sanity_raw_features = latent_to_structure.to_structure(emb, sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945b721c-e677-469e-b862-b3be34c8730b",
   "metadata": {},
   "source": [
    "## Massive Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd96cf-0cfa-47ef-90de-0fb09973508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for fun, let's examine the massive activations:\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_ = sns.heatmap(emb[0].cpu().numpy())\n",
    "_ = plt.ylabel(\"Length index\")\n",
    "_ = plt.xlabel(\"Channel index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5715f23-7f06-456f-ab41-72dfc39055bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chan_means = emb.mean(dim=(0,1))\n",
    "print(chan_means.shape)\n",
    "print(chan_means.max().item(), chan_means.min().item())\n",
    "_ = plt.hist(chan_means.cpu().numpy(), bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e571d87f-6756-43a5-baaf-ce03d782c84d",
   "metadata": {},
   "source": [
    "# Compressed representation\n",
    "\n",
    "**!!! important !!!**\n",
    "The compression model was trained with the assumption that the embedding was per-channel normalized, so we need to per-channel normalize it first.\n",
    "TODO: build out a streamlined interface for doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747a3024-0a0c-4cd0-88ef-61ee8334e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cheap.utils import LatentScaler\n",
    "latent_scaler = LatentScaler()\n",
    "\n",
    "emb_scaled = latent_scaler.scale(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c22646-0d1e-4333-9c4b-c0c275f4841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if only getting the compressed embeddings without requiring the reconstruction, can just use:\n",
    "# compressed_representation, downsampled_mask = model(emb, mask, infer_only=True)\n",
    "# but we would like to evaluate the reconstruction in this example.\n",
    "\n",
    "x_recons, loss, log_dict, compressed_representation, downsampled_mask = model(emb_scaled, mask, infer_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab8ddcb-468e-492e-8a3d-3a28d2d279e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compressed_representation.shape)\n",
    "print(downsampled_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991ccd14-4d5b-4e53-b408-3a24ff7851b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fun, let's look at the activations again\n",
    "# notice that all values are bounded to between [-1, 1]\n",
    "# and the channel and length have been compressed\n",
    "_ = sns.heatmap(compressed_representation[0].cpu().numpy())\n",
    "_ = plt.ylabel(\"Length index\")\n",
    "_ = plt.xlabel(\"Channel index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c26c2fb-4770-40e8-9f29-b41b81dc2d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "recons_unscaled = latent_scaler.unscale(x_recons)\n",
    "\n",
    "recons_sequence = latent_to_sequence.to_sequence(recons_unscaled, mask)[-1]\n",
    "recons_pdb_strs, _ = latent_to_structure.to_structure(recons_unscaled, recons_sequence, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec42998-dcb3-48a2-837c-5db987808ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import py3Dmol\n",
    "\n",
    "for i in range(len(recons_pdb_strs)):\n",
    "    view = py3Dmol.view(width=600, height=600)\n",
    "    view.addModelsAsFrames(sanity_pdb_strs[i])\n",
    "    view.addModelsAsFrames(recons_pdb_strs[i])\n",
    "    view.setStyle({\"model\": 0}, {\"cartoon\": {\"color\": \"gray\", \"opacity\": 0.7}})\n",
    "    view.setStyle({\"model\": 1}, {\"cartoon\": {\"color\": \"red\", \"opacity\": 0.9}})\n",
    "    view.zoomTo()\n",
    "    view.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e710fd-2a58-46e7-ae5d-56194cf5ca9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (plaid)",
   "language": "python",
   "name": "plaid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
